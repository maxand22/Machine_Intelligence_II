---
title: "nonames2"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/maxand22/Google Drive/Humboldt/4. Semester/Machine Intelligence II/Machine_Intelligence_II/")
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```


```{r packages}
#install.packages("jpeg")
library(jpeg)

#install.packages("fastICA")
library(fastICA)

#install.packages("reshape2")
library(reshape2)

#install.packages("ggplot2")
library(ggplot2)

#install.packages("gridExtra")
library(gridExtra)

library(audio)

#install.packages("psych")
library(psych)

```



```{r helper functions}
get_patch <- function(matrix, h, w, return_vector = FALSE){
  h <- h-1
  w <- w-1
  n <- nrow(matrix)
  p <- ncol(matrix)
  h_sample <- sample(1:n, 1, FALSE)
  w_sample <- sample(1:p, 1, FALSE)
      if((h_sample + h) > n) {
          h_patch <- (h_sample - h):h_sample
      }else{
          h_patch <- h_sample:(h_sample + h)
      }
      if((w_sample + w) > p) {
          w_patch <- (w_sample - w):w_sample
      }else{
          w_patch <- w_sample:(w_sample + w)
      }
      if(return_vector == FALSE){ return(matrix[h_patch, w_patch])
      }else{
      return(as.vector(matrix[h_patch, w_patch]))
      }
}


heatmap_custom <- function(matrix){
  g1 <- ggplot(data = matrix, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  scale_fill_continuous(low = "white", high = "black") + guides(fill = FALSE) + theme(axis.title.x=element_blank(),
  axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
  
  return(g1) 
}

norm_vec <- function(x) sqrt(sum(x^2))

g1 <- function(x) {tanh(x)}


```


Ex.2
```{r}
### Load Data
s1 = read.table("hw5/sound1.dat", header=FALSE)
s2 = read.table("hw5/sound2.dat", header=FALSE)

S = t(as.matrix(data.frame(s1,s2)))

# whiten S
St <- t(S)
covS <- cov(scale(St, center = TRUE, scale = FALSE))
ev <- eigen(covS)$vectors
eval <- eigen(covS)$values
Zt = scale(St, center = TRUE, scale = FALSE) %*% ev %*% diag((eval)^-0.5)
cov(Zt)
```

```{r}
# whitened Data: Z
Z <- t(Zt)

# initialize mixing Matrix A and compute Mixed Signal X
set.seed(1234)
A = matrix(runif(4, 0, 1), nrow = 2)

#mixted signals
X = A%*%Z

```


# one unit alogorithm (nur eine independent component wird extrahiert)
```{r}
# init random vector with length 1
vec <- matrix(c(0,1), nrow = 2, ncol = 1)

# inital learning rate
eta <- 0.01

for(i in 1:18000){
  
  eta <- eta*0.9999
    
  deltaVec <- eta*(-1) * X[,i] * g1(t(vec)%*%X[,i])
  
  vec <- vec + deltaVec
  
  vec <- vec/norm_vec(vec)
}

```

```{r}
#check
shat <- t(vec) %*% X
plot(t(shat), type = 'l')
plot(1:18000, t(s1),type = 'l')
plot(1:18000, t(s2),type = 'l')
play(audioSample(as.matrix(shat), rate = 8192))

```


##### multiple unit alogorithm (beide independent components sollen extrahiert werden)

```{r}

# init random matrix (two orthogonal vectors, each with length 1)
set.seed(123)
b1 <- matrix(runif(2, min = 0, max = 1), nrow = 2, ncol = 1)
b1 <- b1/norm_vec(b1)
b2 <- matrix(c(1, (-b1[2]/b1[1])), nrow = 2, ncol = 1)
b2 <- b2/norm_vec(b2)

#initialized unmixing matrix
B <- cbind(b1,b2)

# inital learning rate
eta2 <- 0.01

for(i in 1:18000){
  #learning rate
  eta2 <- eta2*0.9999
  
  #update b1
  deltaB1 <- eta2*(-1) * X[,i] * g1(t(b1)%*%X[,i])
  b1 <- b1 + deltaB1
  
  #update b2
  deltaB2 <- eta2*(-1) * X[,i] * g1(t(b2)%*%X[,i])
  b2 <- b2 + deltaB2
  
  #recalculate B
  B <- cbind(b1,b2)
  
  #normalize B
  maxNorm <- max(norm_vec(b1), norm_vec(b2))
  B <- B/maxNorm
  
  #decorrelate: FUNKTIONIERT NICHT! B danach linear abh?ngig
  Q <- eigen(cov(B%*%t(B)))$vectors
  EV <- eigen(cov(B%*%t(B)))$values
  Lambda <- diag(EV^-0.5)
  B <- Q %*% Lambda %*% t(Q) %*% B
  
  #normalize B
  #b1 <- B[,1]
  #b2 <- B[,2]
  #maxNorm <- max(norm_vec(b1), norm_vec(b2))
  #B <- B/maxNorm
  #b1 <- b1/norm_vec(b1)
  #b2 <- b2/norm_vec(b2)
  
  #B <- ((B%*%t(B))^-.5) %*% B
}

B
```


The file imgpca.zip (used also in exercise sheet 2) contains three categories of images: na- ture, buildings, and text (prefixes n,b,t). For each category:
```{r load images}
path <- "/Users/maxand22/Google Drive/Humboldt/4. Semester/Machine Intelligence II/Machine_Intelligence_II/hw7/imgpca"


fn <- file.path(path, c("n1.jpg","n2.jpg", "n3.jpg", "n4.jpg", "n5.jpg", "n6.jpg", "n7.jpg", "n8.jpg", "n9.jpg", "n10.jpg","n11.jpg","n12.jpg","n13.jpg"))
n <- lapply(fn, readJPEG)

fb <- file.path(path, c("b1.jpg","b2.jpg", "b3.jpg", "b4.jpg", "b5.jpg", "b6.jpg", "b7.jpg", "b8.jpg", "b9.jpg", "b9_2.jpg","b9_4.jpg","b9_8.jpg","b9_16.jpg","b10.jpg"))
b <- lapply(fb, readJPEG)

ft <- file.path(path, c("t1.jpg","t2.jpg", "t3.jpg", "t4.jpg", "t5.jpg", "t6.jpg", "t7.jpg", "t8.jpg", "t9.jpg", "t10.jpg","t11.jpg","t12.jpg","t13.jpg","t14.jpg"))
t <- lapply(ft, readJPEG)
```

(a) Sample P patches of N × N pixels from all images of this category and rearrange
each sample to a column vector. Choose number and size of the patches according to your computing resources. Recommended are P ≥ 20000 and N ≥ 144.
```{r 7.3.a}
N <- sqrt(144)
P <- 20000

matrix_n <- matrix(nrow = 0, ncol = N*N)
matrix_b <- matrix(nrow = 0, ncol = N*N)
matrix_t <- matrix(nrow = 0, ncol = N*N)

set.seed(123)
for(i in 1:length(n)){ 
  tmp1 <- t(replicate(2000, get_patch(n[[i]], N, N, return_vector = TRUE), simplify = "vector"))
  matrix_n <- rbind(matrix_n, tmp1)
}

for(i in 1:length(b)){ 
  tmp1 <- t(replicate(2000, get_patch(b[[i]], N, N, return_vector = TRUE), simplify = "vector"))
  matrix_b <- rbind(matrix_b, tmp1)
}

for(i in 1:length(t)){ 
  tmp1 <- t(replicate(2000, get_patch(t[[i]], N, N, return_vector = TRUE), simplify = "vector"))
  matrix_t <- rbind(matrix_t, tmp1)
}



#The required 20000 samples were reached
#for nature
P < nrow(matrix_n)
#for buildungs
P < nrow(matrix_b)
#for
P < nrow(matrix_t)

```


(b) Calculate the independent features of the image patches (these are the columns of mixing matrix A). Use a fastICA toolbox to compute this matrix:
  • Let fastica perform PCA and whitening of the data.
  • Use the contrast function G(sˆ) = 1 log cosh(asˆ) with a = 1.
```{r 7.3.b}

set.seed(1234)
  
a_n <- fastICA(matrix_n, N*N, alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "R", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)

a_b <- fastICA(matrix_b, N*N, alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "R", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)

a_t <- fastICA(matrix_t, N*N, alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "R", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)

```

whitening:
use pre-whitening matrix that projects data onto the first n.comp principal components
and check
```{r 7.3.b check}
proj_b = matrix_b %*% a_b$K
proj_n = matrix_n %*% a_n$K
proj_t = matrix_t %*% a_t$K

# check results

# plot heatmaps
ggplot(data = melt(cov(matrix_b)), aes(x=Var1, y=Var2, fill=value)) + 
    geom_tile() + 
    scale_fill_gradient(high = "darkred", low = "white") + 
    ggtitle("Cov matrix: buildings after whitening")

ggplot(data = melt(cov(matrix_n)), aes(x=Var1, y=Var2, fill=value)) + 
    geom_tile() + 
    scale_fill_gradient(high = "darkred", low = "white") + 
    ggtitle("Cov matrix: nature after whitening")

ggplot(data = melt(cov(matrix_t)), aes(x=Var1, y=Var2, fill=value)) + 
    geom_tile() + 
    scale_fill_gradient(high = "darkred", low = "white") + 
    ggtitle("Cov matrix: text after whitening")
```


(c) Show the first 20 independent features as (grayscale) image patches by rearranging the vec-
√√
tors into N × N matrices and compare the results for the different categories. Order
  the independent features by decreasing negentropy, (such that the first feature has largest (approximated) negentropy etc).
```{r 7.3.c}

#the independent factors in the right direction
A_n <- a_n$A[,1:20] 
A_b <- a_b$A[,1:20]
A_t <- a_t$A[,1:20]

ica_patchesn <- ica_patchesb <- ica_patchest <- vector("list", length = 20)

for(i in 1:20){
  ica_patchesn[[i]] <- melt(matrix(A_n[,i], N, N))
  ica_patchesb[[i]] <- melt(matrix(A_b[,i], N, N))
  ica_patchest[[i]] <- melt(matrix(A_t[,i], N, N))
}

plotlist_n <- lapply(ica_patchesn, heatmap_custom)
do.call("grid.arrange", c(plotlist_n, ncol=5))

plotlist_b <- lapply(ica_patchesb, heatmap_custom)
do.call("grid.arrange", c(plotlist_b, ncol=5))

plotlist_t <- lapply(ica_patchest, heatmap_custom)
do.call("grid.arrange", c(plotlist_t, ncol=5))
```


(d) Perform PCA on the same set of patches, plot the the principal components (ordered by decreasing eigenvalue) as in (c) and compare them with the independent features.
```{r 7.3.d}
#center
matrix_n_centered <- scale(matrix_n, center = TRUE, scale = FALSE) 
matrix_b_centered <- scale(matrix_b, center = TRUE, scale = FALSE)
matrix_t_centered <- scale(matrix_t, center = TRUE, scale = FALSE) 

#PCA's
pcs_n <- prcomp(matrix_n_centered)$rotation
pcs_b <- prcomp(matrix_b_centered)$rotation 
pcs_t <- prcomp(matrix_t_centered)$rotation 

pca_patchesn <- pca_patchesb <- pca_patchest <- vector("list", length = 20)

for(i in 1:20){
  pca_patchesn[[i]] <- melt(matrix(pcs_n[,i], N, N))
  pca_patchesb[[i]] <- melt(matrix(pcs_b[,i], N, N))
  pca_patchest[[i]] <- melt(matrix(pcs_t[,i], N, N))
}


plotlist_n2 <- lapply(pca_patchesn, heatmap_custom)
do.call("grid.arrange", c(plotlist_n2, ncol=5))

plotlist_b2 <- lapply(pca_patchesb, heatmap_custom)
do.call("grid.arrange", c(plotlist_b2, ncol=5))

plotlist_t2 <- lapply(pca_patchest, heatmap_custom)
do.call("grid.arrange", c(plotlist_t2, ncol=5))


```